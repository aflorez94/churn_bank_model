### MODELO RANDOM FOREST    #####
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import RocCurveDisplay
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    confusion_matrix, classification_report,
    mean_absolute_error, mean_squared_error, roc_auc_score, RocCurveDisplay
)
import lightgbm as lgb
from shapash.explainer.smart_explainer import SmartExplainer

#csv
df = pd.read_csv("/Users/aflorez/Downloads/BankChurners.csv")

df = df.drop(columns=[
    'CLIENTNUM',
    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',
    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'
])

# Target Binaria
df['Attrition_Flag'] = df['Attrition_Flag'].apply(lambda x: 1 if x == 'Attrited Customer' else 0)

# encoding de variables categóricas
X = pd.get_dummies(df.drop(columns='Attrition_Flag'), drop_first=True)
y = df['Attrition_Flag']

#### TRAIN #####
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)


y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

### DESEMPEÑO DEL MODELO ######
conf_matrix_rf = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = np.mean(np.abs((y_test - y_pred) / np.maximum(y_test, 1))) * 100
auc = roc_auc_score(y_test, y_proba)

print("Matriz de Confusión:\n", conf_matrix_rf)
print("\nReporte de Clasificación:\n", report)
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAPE: {mape:.2f}%")
print(f"AUC ROC: {auc:.4f}")

resultados = X_test.copy()
resultados['Real'] = y_test.values
resultados['Predicho'] = y_pred
resultados['Probabilidad_Attrited'] = y_proba
resultados.to_csv("predicciones_random_forest.csv", index=False)


# Matriz de confusión
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix_rf, annot=True, fmt="d", cmap="Blues",
           xticklabels=["Existing", "Attrited"], yticklabels=["Existing", "Attrited"])
plt.title("Matriz de Confusión")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.tight_layout()
plt.show()

# Curva ROC
RocCurveDisplay.from_predictions(y_test, y_proba)
plt.title("Curva ROC - Random Forest")
plt.grid(True)
plt.show()



#### MODELO DE LIGHTGBM   ######

df = pd.read_csv("/Users/aflorez/Downloads/BankChurners.csv")
df = df.drop(columns=[
    'CLIENTNUM',
    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',
    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'
])

# target
df['Attrition_Flag'] = df['Attrition_Flag'].apply(lambda x: 1 if x == 'Attrited Customer' else 0)

X = pd.get_dummies(df.drop(columns='Attrition_Flag'), drop_first=True)
y = df['Attrition_Flag']

### TRAIN  ####
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

model = lgb.LGBMClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

####### DESEMPEÑO #######
conf_matrix = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = np.mean(np.abs((y_test - y_pred) / np.maximum(y_test, 1))) * 100
auc = roc_auc_score(y_test, y_proba)


print("Matriz de Confusión:\n", conf_matrix)
print("\nReporte de Clasificación:\n", report)
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAPE: {mape:.2f}%")
print(f"AUC ROC: {auc:.4f}")

# Matriz de confusión
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
           xticklabels=["Existing", "Attrited"], yticklabels=["Existing", "Attrited"])
plt.title("Matriz de Confusión - LightGBM")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.tight_layout()
plt.show()

#ROC
RocCurveDisplay.from_predictions(y_test, y_proba)
plt.title("Curva ROC - LightGBM")
plt.grid(True)
plt.show()

importances = model.feature_importances_ #IMPORTANCIAS DE VARIABLES
features = X.columns
importances_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)


importances = model.feature_importances_
features = X.columns
importances_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

top_features = importances_df.head(20)
plt.figure(figsize=(10, 8))
plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1])
plt.xlabel("Importancia")
plt.title("Top 20 Feature Importances - LightGBM")
plt.tight_layout()
plt.show()

# Guardar predicciones
resultados = X_test.copy()
resultados['Real'] = y_test.values
resultados['Predicho'] = y_pred
resultados['Probabilidad_Attrited'] = y_proba
resultados.to_csv("predicciones_lightgbm.csv", index=False)

#classifier = LGBMClassifier(**MODEL.get_params()).fit(X_master, y_master)
#classifier._Booster = MODEL.booster_
#y_pred = pd.DataFrame(classifier.predict_proba(X_master))
#y_pred["_predict_"] = y_pred[1]
#y_pred = y_pred["_predict_"]


#xpl = shapash_dashboard(X_master, classifier, cols, y_pred)
#app = xpl.run_app()
#app.kill()
# Explicador Shapash
y_pred_series = pd.Series(model.predict(X_test), index=X_test.index)
xpl = SmartExplainer(model=model)
xpl.compile(
    x=X_test,
    y_pred=y_pred_series
)

# Mostrar interfaz web interactiva
xpl.run_app()
